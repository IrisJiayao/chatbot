import openai
import json
from transformers import AutoTokenizer, AutoModelForCausalLM, TextGenerationPipeline
import torch

client = openai.OpenAI(api_key="")
# ==============================
# ğŸ”¹ GPTï¼ˆOpenAIï¼‰è°ƒç”¨å‡½æ•°
# ==============================
def call_openai(prompt, model="gpt-3.5-turbo", temperature=0.2, max_tokens=1024):
    try:

        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
            max_tokens=max_tokens,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"[OpenAI Error] {e}"

# ==============================
# ğŸ”¹ DeepSeek è°ƒç”¨å‡½æ•°ï¼ˆæœ¬åœ°æ¨ç†ï¼‰
# ==============================
def call_deepseek(prompt, model_name="deepseek-ai/deepseek-llm-1.3b-chat"):
    try:
        # æ¨èåœ¨é¦–æ¬¡è°ƒç”¨å¤–éƒ¨é¢„è½½
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.float16)
        model = model.to("cuda" if torch.cuda.is_available() else "cpu")
        pipe = TextGenerationPipeline(model=model, tokenizer=tokenizer)

        output = pipe(prompt, max_new_tokens=512, do_sample=True)[0]["generated_text"]
        return output.strip()
    except Exception as e:
        return f"[DeepSeek Error] {e}"

# ==============================
# ğŸ”¹ ç»Ÿä¸€å…¥å£ï¼šcall_llm
# ==============================
def call_llm(prompt, model="gpt-3.5", **kwargs):
    print(f"\n[LLM] Calling model: {model}")

    if model.startswith("gpt"):
        return call_openai(prompt, model=model, **kwargs)

    elif model == "deepseek":
        return call_deepseek(prompt)

    else:
        return f"[LLM Error] Unsupported model: {model}"

def load_prompt_template(name: str, input_dict: dict) -> str:
    """
    åŠ è½½ prompts/name.txt å¹¶å°† {{KEY}} æ›¿æ¢æˆ input_dict["KEY"]
    """
    try:
        path = f"prompt/{name}.txt"
        with open(path, "r", encoding="utf-8") as f:
            template = f.read()
        for key, value in input_dict.items():
            template = template.replace(f"{{{{{key}}}}}", str(value))
        return template
    except Exception as e:
        return f"[Prompt åŠ è½½å¤±è´¥: {e}]"

def call_llm_conversational(user_input: str, session_state: dict, model: str = "gpt") -> str:
    """
    è°ƒç”¨ GPT/LLM å¹¶ä¿ç•™å†å²å¤šè½®å¯¹è¯ã€‚
    """
    try:
        # åˆå§‹åŒ–å¯¹è¯å†å²
        if "history" not in session_state:
            session_state["history"] = []

        # æ·»åŠ ç”¨æˆ·è¾“å…¥
        session_state["history"].append({"role": "user", "content": user_input})

        # è°ƒç”¨ GPT
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",  # or other
            messages=session_state["history"],
            temperature=0.4,
            max_tokens=400
        )

        # æå– assistant å›å¤å¹¶åŠ å…¥å†å²
        reply = response.choices[0].message.content.strip()
        session_state["history"].append({"role": "assistant", "content": reply})
        return reply

    except Exception as e:
        return f"âš ï¸ Error: {e}"

def finalize_prediction_parameters(extracted: dict, context: dict) -> dict:
    """
    ç”¨å†å²ä¸Šä¸‹æ–‡è¡¥å…¨æå–å‚æ•°ä¸­ç¼ºå¤±çš„éƒ¨åˆ†ã€‚

    Args:
        extracted (dict): æ¥è‡ª extract_parameters_from_nl çš„ LLM æå–ç»“æœï¼ˆå¯èƒ½æœ‰ç©ºå­—æ®µï¼‰
        context (dict): ä¸Šä¸€æ¬¡é¢„æµ‹çš„å®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆlast_prediction_contextï¼‰

    Returns:
        dict: æœ€ç»ˆé¢„æµ‹å‚æ•°ï¼ˆdataset, analysis_type, target_ip, time_pointï¼‰
    """
    return {
        "dataset": extracted.get("dataset") or context.get("dataset"),
        "analysis_type": extracted.get("analysis_type") or context.get("analysis_type"),
        "target_ip": extracted.get("target_ip") or context.get("target_ip"),
        "time_point": extracted.get("time_point") or context.get("time_point"),
    }


def generate_caption_from_chart(dataset, ip, time_point, model="gpt"):
    prompt = load_prompt_template("chart_caption", {
        "dataset": dataset,
        "ip": ip,
        "time": time_point
    })
    return call_llm(prompt, model=model)

def summarize_multiple_predictions(result_list, model="gpt"):
    formatted_records = ""
    for r in result_list:
        formatted_records += f"- IP: {r['target_ip']}, Time: {r['time_point']}, Predicted: {r['prediction']}, Actual: {r['actual']}\n"

    prompt = load_prompt_template("summarize_multiple_predictions", {
        "records": formatted_records
    })
    return call_llm(prompt, model=model)
